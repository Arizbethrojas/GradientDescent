# -*- coding: utf-8 -*-
"""HW1_Winter24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10KvRMRbmB1IooL6-Qi3p8Zr_dAUjYZWb

# HW1  Gradient descent (Due: January 19, 2024, 11:59 PM)

Please submit this Jupyter notebook with your solutions. The solutions should include the code, explanations, and the output of all the cells. Submitting your solutions without running your code will lead to a deduction of points.

For problems that refer to a cost function without specifying it, assume the cost function from the previous question.

Also, note that you should cite all the references you refer to under each question. Proper referencing is essential for academic integrity, giving credit to original authors, avoiding plagiarism, and providing a traceable path for verification. Please check the course syllabus for more details about academic integrity.

Your name:

0) [0 points] You are allowed to engage in discussions about homework assignments; however, it is crucial that you independently write down your own code and solutions. Additionally, if you discuss the assignment with other students, please explicitly mention their names below.

Akshee Chopra

Students you have discussed with:

1) [5 points] Calculate the derivative of following cost function and write it down:

$g(w) = \frac{1}{33}\left(w^3 + w^2 + 33w - 33 \right)$

$\frac{\partial}{\partial w}g(w) = \frac{1}{33}(3w^2 + 2w +33)$

2) [25 points] Implement the gradient descent function as discussed in class using the gradient derived in the last problem. The function should return the cost history for each step. Use the code template below:
"""

#gradient descent function
#inputs: alpha (learning rate parameter), max_its (maximum number of iterations), w0 (initialization)
def gradient_descent(alpha, max_its, w0):
  prev_w = w0
  cost_history = []
  for i in range(max_its):
      curr_w = prev_w - alpha * ((1/33) * ((3 * (prev_w**2))+(2 * prev_w)+ 33))
      curr_cost = ((1/33)*((curr_w**3)+(curr_w**2)+(33*curr_w) - 33))
      cost_history.append(curr_cost)
      prev_w= curr_w

  return cost_history

"""3) [10 points] Run the gradient_descent function you implemented three times, with the following parameters. Generate a single plot showing the cost as a function of step number for all three runs (combine all three runs into a single plot). If you are not familiar with plotting in python, here is the docs for matplotlib:(https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot).


$w^0$ = 2.95
max_its = 500

# first run
alpha = 0.01
# second run
alpha = 0.005
# third run
alpha = 0.001

"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np

cost_history_1 = gradient_descent(0.01, 500, 2.95)
cost_history_2 = gradient_descent(0.005, 500, 2.95)
cost_history_3 = gradient_descent(0.001, 500, 2.95)

x_axis = list(range(1, 501))
y_1 = np.array(cost_history_1)
x = np.array(x_axis)
plt.plot(x, y_1, color= "red")

y_2 = np.array(cost_history_2)
plt.plot(x, y_2, color= "blue")

y_3 = np.array(cost_history_3)
plt.plot(x, y_3, color= "green")

plt.title("Gradient Descent for Varying Learning Rates")
plt.xlabel("iterations")
plt.ylabel("cost")

plt.plot(x, y_1, "-r", label="alpha is 0.01")
plt.plot(x, y_2, "-b", label="alpha is 0.005")
plt.plot(x, y_3, "-g", label="alpha is 0.001")

plt.legend(loc = "upper right")

plt.show()

"""For the next few problems we will be comparing fixed and diminishing learning rates

Take the following cost function:
\begin{equation}
g(w) = |w|
\end{equation}

4) [5 points] Is this function convex? If no, why not? If yes, where is its global minimum?

To know if a function is convex, we take its second derivative and see if it has positive values for all x values. The derivative of an absolute value function has a discontinuity at 0, so it becomes a piece wise function. Otherwise the function is convex. Its global minimum is at 0.

5) [5 points] What is the derivative of the cost function?

\begin{equation}
g'(w) = \frac{w}{|w|}\
\end{equation}

6) [20 points] Rewrite the gradient descent function from question 2 such that it takes the cost funciton g as input and uses the autograd library to calculate the gradient. The function should return the weight and cost history for each step. Use the code template below.

autograd is a python package for automatic calculation of the gradient. Here is a tutorial on it: (http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/tutorials/tut4.pdf

Note that in Python you can pass functions around like any other variables. That is why you can pass the cost function g to the gradient_descent function.

You should be able to install it by running "pip install autograd" in a cell in your Jupyter notebook.
"""

from autograd import grad

#gradient descent function
#inputs: g (cost function), alpha (learning rate parameter), max_its (maximum number of iterations), w (initialization)
def gradient_descent(g,alpha,max_its,w0):
    gradient = grad(g)   ## This is how you use the autograd library to find the gradient of a function
    weight = w0;
    weight_history = []
    cost_history = []
    for i in range(max_its):

      curr_w = weight - alpha * gradient(weight)
      curr_cost = g(curr_w)
      cost_history.append(curr_cost)
      weight_history.append(curr_w)
      weight = curr_w

    return weight_history,cost_history

"""7) [10 points] Make a run of max_its=30 steps of gradient descent with initialization at the point $w^0 = 1.6$, and a fixed learning rate of $\alpha = 0.5$. Using the cost and weight history, plot the cost as a function of the weight for each step (cost on y-axis, weight on x-axis). Recall that the terms weight and parameter used interchangeably and both refer to w."""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
cost_function = abs
weight_array, cost_array = gradient_descent(cost_function, 0.5, 30, 1.6)
y = np.array(cost_array)
x = np.array(weight_array)
plt.plot(x, y, color= "red")
plt.xlabel("weight")
plt.ylabel("cost")

plt.title("Gradient Descent ")

plt.show()

"""8) [15 points] Make a run of max_its=30 steps of gradient descent with initialization at the point $w^0 = 1.6$, using the diminishing rule $\alpha = \frac{1}{k^{1.5}}$ (for this you have to modify the gradient_descent function slightly. Use the code template below. Using the cost and weight history, plot the cost as a function of the weight for each step (cost on y-axis, weight on x-axis)"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
from autograd import grad

#gradient descent function
#inputs: g (cost function), alpha (learning rate parameter), max_its (maximum number of iterations), w (initialization)
def gradient_descent(g,alpha,max_its,w0):
    gradient = grad(g)   ## This is how you use the autograd library to find the gradient of a function
    weight = w0;
    weight_history = [g(w0)]
    cost_history = [w0]
    for i in range(max_its):
      if alpha=='diminishing':
        new_alpha = 1/ ((i+1)**1.5)
        curr_w = weight - new_alpha * gradient(weight)
        curr_cost = g(curr_w)
        cost_history.append(curr_cost)
        weight_history.append(curr_w)
        weight=curr_w
      else:
        curr_w = weight - alpha * gradient(weight)
        curr_cost = g(curr_w)
        cost_history.append(curr_cost)
        weight_history.append(curr_w)
        weight=curr_w

    return weight_history,cost_history

cost_function = abs
    weight_array, cost_array = gradient_descent(cost_function, "diminishing", 30, 1.6)
    y = np.array(cost_array)
    x = np.array(weight_array)
    plt.plot(x, y, color= "red")
    plt.xlabel("weight")
    plt.ylabel("cost")
    plt.title("Gradient Descent: Absolute Value")

"""9) [10 points]  Generate a single plot showing the cost as a function of step number for both runs (combine all  runs into a single plot). Which approach works better? Why?

Diminishing rate is a better approach because it oscillates less and reaches the minimum in less iterations
"""

import matplotlib.pyplot as plt
cost_function = abs
dim_alpha_weight, dim_alpha_cost = gradient_descent(cost_function, "diminishing", 30, 1.6)
weight_array, cost_array = gradient_descent(cost_function, 0.5, 30, 1.6)

dim_y = np.array(dim_alpha_cost)
x_axis = list(range(0, 31))
x = np.array(x_axis)
y = np.array(cost_array)
plt.plot(x, dim_y, color= "red")
plt.plot(x, y, color= "blue")
plt.xlabel("weight")
plt.ylabel("cost")
plt.title("Diminishing vs. Fixed alpha")

plt.plot(x, dim_y, "-r", label="alpha is diminishing")
plt.plot(x, y, "-b", label="alpha is fixed")

plt.legend(loc = "upper right")
plt.show()

"""We will now look at the oscilating behavior of gradient descent.

Take the following cost function:
$g(w) = w_0^2 + w_1^2 + 3\sin(w_0 + 3w_1) + 10$

Note that this cost function has two parameters.

10) [5 points] Make sure your gradient descent function from problem 6 can handle cost functions with more than one parameter. You may need to rewrite it if you were not careful. Use the code template below (if your function from problem 6 is good, you can just copy and paste it here)
"""

from autograd import grad
import autograd.numpy as np

#gradient descent function
#inputs: g (cost function), alpha (learning rate parameter), max_its (maximum number of iterations), w (initialization)

def modified_g (temp_w):
  return temp_w[0]**2 + temp_w[1]**2 +3* np.sin((temp_w[0]+(3 * temp_w[1]))) +10

def gradient_descent(g,alpha,max_its,w0):
    gradient = grad(g)   ## This is how you use the autograd library to find the gradient of a function
    weight = w0;
    weight_history = []
    cost_history = []
    for i in range(max_its):

      curr_w = weight - alpha * gradient(weight)
      curr_cost = g(curr_w)
      cost_history.append(curr_cost)
      weight_history.append(curr_w)
      weight = curr_w
      i = i +1

    return weight_history,cost_history

"""11) [10 points] Run the gradient_descent function with the cost function above three times with the following parameters. Generate a single plot showing the cost as a function of step number for all three runs (combine all three runs into a single plot). Use the code template below. Which alpha leads to an oscillating behavior?

$w^0$ = [10.0,-10.0]
max_its = 30

# first run
alpha = 0.01
# second run
alpha = 0.1
# third run
alpha = 1


"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import autograd.numpy as np

w0 = np.array([10.0, -10.0])
weight_history1, cost_history_1 = gradient_descent(modified_g, 0.01, 30, w0)
weight_history2, cost_history_2 = gradient_descent(modified_g, 0.1, 30, w0)
weight_history3, cost_history_3 = gradient_descent(modified_g, 1, 30, w0)

x_axis = list(range(0, 30))

y_1 = np.array(cost_history_1)
x = np.array(x_axis)

plt.plot(x, y_1, color= "red")

y_2 = np.array(cost_history_2)
plt.plot(x, y_2, color= "blue")

y_3 = np.array(cost_history_3)
plt.plot(x, y_3, color= "green")

plt.xlabel("iterations")
plt.ylabel("cost")
plt.title("Various Learning Rates")

plt.plot(x, y_1, "-r", label="alpha is 0.01")
plt.plot(x, y_2, "-b", label="alpha is 0.1")
plt.plot(x, y_3, "-g", label="alpha is 1")

plt.legend(loc = "upper right")

plt.show()

"""12) [15 points] This problem is about learning to tune fixed step length for gradient descent. Here, you are given a cost function:
$g(w) = 3w_0^2 + 2w_1^2 +3w_2^2$

Assume your $w^0$= [1,2,1] and your max_iter = 100

Use your latest gradient descent function with a fixed learning rate. Play around with at least 5 different values of alpha. Generate a single plot of the cost as a function of the number of iterations. What was your intuition for the selection of alpha values? Which value of alpha seems to converge the fastest?

Note that your grade will not depend on how well you do, as long as you try at least 5 different values for alpha and plot them.

My intuition for selecting learing rates was based on magnitude and numbers to the power of 10. The largest learning rate converges the fastest, which makes sense with what we learned in class. The downfall of its speed is that a large alpha can often miss the minimum by overlooking it.
"""

from autograd import grad
import autograd.numpy as np

#gradient descent function
#inputs: g (cost function), alpha (learning rate parameter), max_its (maximum number of iterations), w (initialization)

def modified_g (temp_w):
  return 3* temp_w[0]**2 + 2*temp_w[1]**2 +3* temp_w[2]**2

def gradient_descent(g,alpha,max_its,w0):
    gradient = grad(g)   ## This is how you use the autograd library to find the gradient of a function
    weight = w0;
    weight_history = []
    cost_history = []
    for i in range(max_its):

      curr_w = weight - alpha * gradient(weight)
      curr_cost = g(curr_w)
      cost_history.append(curr_cost)
      weight_history.append(curr_w)
      weight = curr_w
    return weight_history,cost_history

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import autograd.numpy as np

w0 = np.array([1.0,2.0,1.0])

weight_history1, cost_history_1 = gradient_descent(modified_g, 0.1, 100, w0)
weight_history2, cost_history_2 = gradient_descent(modified_g, 0.01, 100, w0)
weight_history3, cost_history_3 = gradient_descent(modified_g, 0.001, 100, w0)
weight_history4, cost_history_4 = gradient_descent(modified_g, 0.0001, 100, w0)
weight_history5, cost_history_5 = gradient_descent(modified_g, 0.00001, 100, w0)

x_axis = list(range(0, 100))

y_1 = np.array(cost_history_1)
x = np.array(x_axis)

plt.plot(x, y_1, color= "red")

y_2 = np.array(cost_history_2)
plt.plot(x, y_2, color= "blue")

y_3 = np.array(cost_history_3)
plt.plot(x, y_3, color= "green")

y_4 = np.array(cost_history_4)
plt.plot(x, y_3, color= "yellow")

y_5 = np.array(cost_history_5)
plt.plot(x, y_3, color= "orange")

plt.title("Gradient Descent for Varying Learning Rates")
plt.xlabel("iterations")
plt.ylabel("cost")

plt.plot(x, y_1, "-r", label="alpha is 0.1")
plt.plot(x, y_2, "-b", label="alpha is 0.01")
plt.plot(x, y_3, "-g", label="alpha is 0.001")
plt.plot(x, y_4, "-y", label="alpha is 0.0001")
plt.plot(x, y_5, "-0", label="alpha is 0.00001")

plt.legend(loc = "upper right")

plt.show()